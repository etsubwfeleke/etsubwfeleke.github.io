[{"categories":["Python","PyTorch","Deep Learning"],"content":"CNN, Data Handling and MINST dataset In part one we learned basics of pytorch and built a simple linear model. In this part we will build a CNN model, learn how to handle data and use the MINST dataset. ","date":"2025-06-13","objectID":"/itracker_cnn/:1:0","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Python","PyTorch","Deep Learning"],"content":"Convolutional Neural Networks (CNNs) CNNs are a type of neural network that are particularly well-suited for image data. They use convolutional layers to automatically and adaptively learn spatial hierarchies of features from input images. This makes them very effective for tasks such as image classification, object detection, and more. ","date":"2025-06-13","objectID":"/itracker_cnn/:1:1","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Python","PyTorch","Deep Learning"],"content":"A Quick Overview of CNNs and Data Handling in PyTorch Convolutional Neural Networks (CNNs) are powerful tools for image-related tasks, leveraging convolutional layers to learn spatial hierarchies of features. They reduce computational complexity through parameter sharing and process images hierarchically, making them robust to spatial variations. Key Components of CNNs: Convolutional Layer (nn.Conv2d): Detects local features using kernels, strides, and padding. It processes input channels and outputs feature maps. Pooling Layer (nn.MaxPool2d): Downsamples feature maps, reducing parameters and enhancing translational invariance. Fully Connected Layer (nn.Linear): Flattens high-level features for final classification or regression tasks. What are CNNs and Why Use Them? Convolutional Neural Networks (CNNs) are a specialized kind of neural network designed for processing data with a grid-like topology — like images. Unlike traditional fully connected layers, CNNs use convolutional layers that scan small regions (kernels) over the input, capturing important features such as edges, textures, and patterns. Key benefits of CNNs include: Parameter sharing: The same kernel (filter) is applied across the entire image, drastically reducing the number of parameters compared to fully connected layers. Spatial hierarchy: CNNs can learn hierarchical features from simple edges to complex objects as you go deeper. Robustness: Pooling layers reduce spatial size, making the model more resilient to small shifts or distortions in the image. ","date":"2025-06-13","objectID":"/itracker_cnn/:1:2","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Python","PyTorch","Deep Learning"],"content":"CNN Layers: nn.Conv2d and nn.MaxPool2d Concept Explanation: Convolutional Layers (nn.Conv2d): The core building block of CNNs. They apply a convolution operation to the input, sliding a small filter (kernel) over the input data. This helps capture local patterns like edges, textures, etc. Key idea: Parameter sharing (the same filter is used across different parts of the input image), which reduces the number of parameters compared to fully connected layers on images. Input: Typically a 4D tensor of shape (N, C_in, H_in, W_in) where: N: Batch size C_in: Number of input channels (e.g., 3 for RGB images, 1 for grayscale) H_in: Height of the input feature map W_in: Width of the input feature map Output: A 4D tensor of shape (N, C_out, H_out, W_out) where C_out is the number of output channels (number of filters used). H_out and W_out depend on kernel size, stride, and padding. Note Key nn.Conv2d Parameters: in_channels (int): Number of channels in the input image. out_channels (int): Number of channels produced by the convolution (number of filters). kernel_size (int or tuple): Size of the convolving kernel. stride (int or tuple, optional): Stride of the convolution. Default: 1. padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0. dilation (int or tuple, optional): Spacing between kernel elements. Default: 1. groups (int, optional): Number of blocked connections from input channels to output channels (for depthwise separable convolutions, etc.). Default: 1. bias (bool, optional): If True, adds a learnable bias to the output. Default: True. Example: ```Python import torch import torch.nn as nn conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) input_tensor = torch.randn(1, 3, 32, 32) # Batch of 1 RGB image 32x32 output = conv_layer(input_tensor) print(output.shape) # Output shape: (1, 16, 32, 32) ``` Pooling Layers (nn.MaxPool2d): Used to reduce the spatial dimensions (height and width) of the feature maps. This helps reduce computation, control overfitting, and makes the network more robust to variations in the position of features. Max Pooling: Takes the maximum value from a small window (kernel) as it slides over the feature map. Input: Typically a 4D tensor from a convolutional layer. Output: A 4D tensor with reduced H and W dimensions. The number of channels C remains the same. Note Key nn.MaxPool2d Parameters: kernel_size (int or tuple): The size of the window to take a max over. stride (int or tuple, optional): The stride of the window. Default value is kernel_size. padding (int or tuple, optional): Implicit zero padding to be added on both sides. Default: 0. dilation (int or tuple, optional): Controls the spacing between the kernel points. Default: 1. return_indices (bool, optional): If True, will return the max indices along with the outputs. Useful for MaxUnpool2d. Default: False. ceil_mode (bool, optional): When True, will use ceil instead of floor to compute the output shape. Default: False. Example: ```python pool_layer = nn.MaxPool2d(kernel_size=2, stride=2) pooled_output = pool_layer(output) print(pooled_output.shape) # Shape: (1, 16, 16, 16) ``` Data Handling in PyTorch: Efficient data handling is crucial for training CNNs. PyTorch provides tools like torchvision.transforms for preprocessing, torch.utils.data.Dataset for custom datasets, and torch.utils.data.DataLoader for batching and shuffling data. Transforms: Convert images to tensors, resize them, and normalize values for effective training. Dataset: Subclass Dataset to load and preprocess data samples. DataLoader: Wraps datasets for batching, shuffling, and parallel loading. These components streamline the process of building and training CNNs for tasks like image classification and gaze tracking. Example of Data Preprocessing with torchvision.transforms: from torchvision import transforms transform = transforms.Compose([ transforms.Resize((\u003cHEIGHT\u003e, \u003cWIDTH\u003e)),# Replace \u003cHEIGHT\u003e an","date":"2025-06-13","objectID":"/itracker_cnn/:1:3","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Python","PyTorch","Deep Learning"],"content":"Custom Dataset and DataLoader To work with your own images or data, subclass torch.utils.data.Dataset and implement: __len__: Returns dataset size. __getitem__: Returns a single sample (image and label), applying transforms. The DataLoader wraps the dataset and provides batching, shuffling, and parallel loading. ","date":"2025-06-13","objectID":"/itracker_cnn/:1:4","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Python","PyTorch","Deep Learning"],"content":"Mini Project: Simple CNN on MNIST Put it all together by building a simple CNN to classify handwritten digits using the MNIST dataset. Load MNIST dataset with transforms (ToTensor, Normalize). Define a CNN with Conv2d, ReLU, MaxPool layers. Train with CrossEntropyLoss and Adam optimizer. Evaluate accuracy on the test set. ","date":"2025-06-13","objectID":"/itracker_cnn/:1:5","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["pytorch","deep learning"],"content":"PyTorch Fundamentals Before delving into the depths of deep learning—pun intended—it’s crucial to grasp the foundational aspects that pave the way. This will be a journal of sorts to document my learning experience as I figure out how to build an eye blink tracker using PyTorch from scratch. ","date":"2025-06-05","objectID":"/itracker/:1:0","tags":null,"title":"PyTorch Eye Blink Tracker Pt.1","uri":"/itracker/"},{"categories":["pytorch","deep learning"],"content":"What is PyTorch? PyTorch is a powerful open-source machine learning library that provides a flexible and efficient platform for building deep learning models. It is particularly well-suited for tasks involving computer vision, natural language processing, and reinforcement learning. PyTorch operates on tensors, akin to numpy arrays but optimized for GPU-accelerated computing. This efficiency boost—up to 100 times faster than CPUs—is due to GPUs’ inherent computational prowess. hugo ","date":"2025-06-05","objectID":"/itracker/:2:0","tags":null,"title":"PyTorch Eye Blink Tracker Pt.1","uri":"/itracker/"},{"categories":["pytorch","deep learning"],"content":"Attributes and Operations: Similar to numpy arrays, tensors support indexing, slicing, reshaping, and mathematical operations. The concept of broadcasting enables simultaneous operations across compatible elements, enhancing computational efficiency. Building Tensors: import torch import numpy as np m = torch.tensor([[1,2,3], [4,5,6]]) print(m) output: tensor([[1, 2, 3], [4, 5, 6]]) Types of Tensors: np_array = np.array([[1,2,3], [7,8,9]]) torch_tensor = torch.from_numpy(np_array) print(f\"Numpy array to tensor: \\n {torch_tensor}\") zeros_tensor = torch.zeros(2,5) print(f\"zeros_tensor: \\n {zeros_tensor}\") one_tesnors = torch.ones(2,5) print(f\"one_tesnors: \\n {one_tesnors}\") rand_tensor = torch.rand(2,3) print(f\" rand_tensor: \\n {rand_tensor}\") randn_tensor = torch.randn(2,3) print(f\"randn_tensor: \\n {randn_tensor}\") float_tensor =torch.tensor([1.0,2.0,3.0]) print(f\"Float tesnor: \\n {float_tensor.dtype}\") int_tensor = torch.tensor([1,2,3,4], dtype=torch.int64) print(f\"Int tensor: \\n {int_tensor.dtype}\") Numpy array to tensor: tensor([[1, 2, 3], [7, 8, 9]]) zeros_tensor: tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) one_tesnors: tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) rand_tensor: tensor([[0.7409, 0.5551, 0.9490], [0.6028, 0.1038, 0.5413]]) randn_tensor: tensor([[-1.3301, -0.3333, -0.4628], [-1.0383, -0.8357, -0.2252]]) Float tesnor: torch.float32 Int tensor: torch.int64 Checking the device and shape of tensors: if torch.cuda.is_available(): device = torch.device(\"cuda\") gpu_tensor = torch.tensor([1,2,3], device=device) print(gpu_tensor.device) else: print(\"CUDA is not available, tensor on CPU\") device = torch.device(\"cpu\") # which is the default print(f\"Shape of tensor: {m.shape}\") print(f\"Datatype of tensor:{m.dtype}\") print(f\"Device tensor is on {m.device}\") CUDA is not available, tensor on CPU Shape of tensor: torch.Size([2, 3]) Datatype of tensor:torch.int64 Device tensor is on cpu PyTorch Tensor operations: extracting rows and columns, changing shapes with .view() and .reshape(), and performing element wise and matrix operations (addition, subtraction, multiplication, division, and matrix multiplication) # 1. Tensor creation and basic indexing/slicing tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) print(\"First row: \", tensor[0]) print(\"First column: \", tensor[:, 0]) print(\"Element at (1,2):\", tensor[1, 2]) # 2. Slicing + in-place update sub = tensor[:2, :2] sub[0, :] = 100 print(\"After in-place update:\\n\", tensor) # 3. Random tensor + reshape/flatten t = torch.rand(4, 4) print(\"Original shape:\", t.shape) print(\"Reshaped (2×8):\", t.view(2, 8).shape) print(\"Flattened: \", t.view(-1).shape) # 4. Element-wise arithmetic x = torch.tensor([1., 2., 3.]) y = torch.tensor([4., 5., 6.]) print(\"\\nAdd: \", x + y) print(\"Sub: \", x - y) print(\"Mul: \", x * y) print(\"Div: \", x / y) # 5. Matrix multiplication mat1 = torch.randn(2, 3) mat2 = torch.randn(3, 4) print(\"\\nMatrix multiplication:\\n\", torch.matmul(mat1, mat2)) # 6. Broadcasting examples a = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(\"\\nAdd scalar: \\n\", a + 10) print(\"Add row vector: \\n\", a + torch.tensor([1, 2, 3])) print(\"Add column vector:\\n\", a + torch.tensor([[1], [2]])) # 7. In-place operation x.add_(y) print(\"\\nIn-place add result:\", x) output: First row: tensor([1, 2, 3]) First column: tensor([1, 4, 7]) Element at (1,2): tensor(6) After in-place update: tensor([[100, 100, 3], [ 4, 5, 6], [ 7, 8, 9]]) Original shape: torch.Size([4, 4]) Reshaped (2×8): torch.Size([2, 8]) Flattened: torch.Size([16]) Add: tensor([5., 7., 9.]) Sub: tensor([-3., -3., -3.]) Mul: tensor([ 4., 10., 18.]) Div: tensor([0.2500, 0.4000, 0.5000]) Matrix multiplication: tensor([[ 0.6673, 0.5182, 1.2387, -0.9455], [ 0.0869, 0.3411, 0.6695, -0.4641]]) Add scalar: tensor([[11, 12, 13], [14, 15, 16]]) Add row vector: tensor([[2, 4, 6], [5, 7, 9]]) Add column vector: tensor([[2, 3, 4], [6, 7, 8]]) In-place add result: tensor([5., 7., 9.]) ","date":"2025-06-05","objectID":"/itracker/:2:1","tags":null,"title":"PyTorch Eye Blink Tracker Pt.1","uri":"/itracker/"},{"categories":["pytorch","deep learning"],"content":"Automatic Differentiation: Central to PyTorch’s appeal is its automatic differentiation capability. By tracking operations on tensors marked with requires_grad=True, it computes gradients (.grad)—crucial for understanding how changes in weights affect model output. Clearing gradients prevents unintended accumulation, ensuring accurate subsequent calculations. Torch.nn.Module: Enter nn.Module, the cornerstone of PyTorch neural networks. This template empowers developers to construct models equipped with essential functionalities—a scaffold for innovation in deep learning architectures. Key Components From common layers and activation functions to loss functions that gauge predictive accuracy, PyTorch offers a robust toolkit for constructing and refining neural networks. Optimizers and Training Loop Optimizers, such as SGD and Adam, fine-tune model parameters by minimizing computed gradients. The training loop—comprising forward passes, loss calculation, backpropagation for gradient computation, and model updates—forms the backbone of neural network training. In essence, mastering these fundamentals equips us to embark on building an eye blink tracker. Stay tuned as we translate theory into practice, harnessing PyTorch’s capabilities to innovate in real-world applications. Gradients \u0026 backward propagation x = torch.tensor(2.0, requires_grad=True) w = torch.tensor(3.0, requires_grad=True) b = torch.tensor(1.0, requires_grad=True) y = w * x + b target = torch.tensor(10.0) loss = (y - target)**2 loss.backward(retain_graph=True) print(f\"Gradient of loss w.r.t x: {x.grad}\") print(f\"Gradient of loss w.r.t w: {w.grad}\") print(f\"Gradient of loss w.r.t b: {b.grad}\") # Reset gradients x.grad = None w.grad = None b.grad = None print(f\"Gradients reset for x, w, b: {x.grad}, {w.grad}, {b.grad}\") # Gradients with no tracking with torch.no_grad(): y_no_grad = w * x + b print(f\"y_no_grad requires_grad: {y_no_grad.requires_grad}\") Gradient of loss w.r.t x: -18.0 Gradient of loss w.r.t w: -12.0 Gradient of loss w.r.t b: -6.0 Gradients reset for x, w, b: None, None, None y_no_grad requires_grad: False Exercise: Gradient computation with new tensors a = torch.tensor(4.0, requires_grad=True) b = torch.tensor(5.0, requires_grad=True) c = torch.tensor(6.0, requires_grad=True) d = a * b + c l = d**2 l.backward() print(f\"Gradient of loss w.r.t a: {a.grad}\") print(f\"Gradient of loss w.r.t b: {b.grad}\") print(f\"Gradient of loss w.r.t c: {c.grad}\") # Verify gradients are correct print(f\"Gradient of loss w.r.t a (dl/da): {2 * d * b}\") print(f\"Gradient of loss w.r.t b (dl/db): {2 * d * a}\") print(f\"Gradient of loss w.r.t c (dl/dc): {2 * d}\") # Reset gradients and calculate for new operations a.grad = None b.grad = None c.grad = None d1 = a * b d1.backward() print(f\"Gradient of d1: {a.grad}, {b.grad}\") Output: Gradient of loss w.r.t a: 260.0 Gradient of loss w.r.t b: 208.0 Gradient of loss w.r.t c: 52.0 Gradient of loss w.r.t a (dl/da): 260.0 Gradient of loss w.r.t b (dl/db): 208.0 Gradient of loss w.r.t c (dl/dc): 52.0 Gradient of d1: 5.0, 4.0 Creating a simple neural network (Linear Regression Model) Essentially, a linear regression model is a simple neural network with one layer and no activation function. It learns to map input features to output values by adjusting weights and biases during training. To create a simple neural network using Pytorch, we can follow the a simple structure or template as follows: class YourModelName(nn.Module): def __init__(self): super().__init__() self.layer1 = ... self.layer2 = ... def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x class LinearRegressionModel(nn.Module): def __init__(self, input_dim, output_dim): super(LinearRegressionModel, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): return self.linear(x) # Instantiate model input_features = 5 output_features = 1 model = LinearRegressionModel(input_features, output_features) # Print model and parameters print(f\"M","date":"2025-06-05","objectID":"/itracker/:2:2","tags":null,"title":"PyTorch Eye Blink Tracker Pt.1","uri":"/itracker/"},{"categories":["pytorch","deep learning"],"content":"Resources: PyTorch Documentation Torch.nn Module Documentation ","date":"2025-06-05","objectID":"/itracker/:2:3","tags":null,"title":"PyTorch Eye Blink Tracker Pt.1","uri":"/itracker/"},{"categories":null,"content":"Let me introduce myself! I’m Etsub Wendwosen, a passionate Data Science graduate student at the University of North Texas (UNT), with a BA in Economics. I have a strong background in data analysis, machine learning, and statistical modeling, and I’m actively expanding my expertise in these areas. My goal is to leverage data science to drive informed decisions and contribute to meaningful projects. ","date":"2025-06-02","objectID":"/about/:1:0","tags":["intro"],"title":"Welcome to My Blog","uri":"/about/"},{"categories":null,"content":"My Skills: Programming Languages: Python, SQL Data Analysis: Pandas, NumPy, Scikit-learn Data Visualization: Tableau, Power BI Machine Learning: Linear Regression, Random Forest, ARIMA Big Data: PySpark, Hadoop Cloud Platforms: AWS, Google Cloud Tools: Jupyter, Git, Docker, VS Code ","date":"2025-06-02","objectID":"/about/:1:1","tags":["intro"],"title":"Welcome to My Blog","uri":"/about/"},{"categories":null,"content":"Current Focus: I’m working on several exciting projects, including: Predicting CO₂ Emissions: Using machine learning models like Linear Regression, Random Forest, and ARIMA to predict CO₂ emissions across different countries using 200 years of historical data. Hotel Reservation Management System: Designing a relational database for hotel management systems to optimize reservations and customer data management. ","date":"2025-06-02","objectID":"/about/:1:2","tags":["intro"],"title":"Welcome to My Blog","uri":"/about/"},{"categories":null,"content":"Interests: I’m a curious learner who loves solving complex problems using data. I’m particularly interested in generative AI, high-performance computing (HPC), and cloud-based solutions. When I’m not coding or analyzing data, I love to dive into game theory, exploring topics like Prisoners’ Dilemma and Nash Equilibrium. ","date":"2025-06-02","objectID":"/about/:1:3","tags":["intro"],"title":"Welcome to My Blog","uri":"/about/"},{"categories":null,"content":"My Academic Journey: Current Program: MS in Data Science (Graduating May 2026) Key Courses: Big Data Analytics Information Organization Parallel Computing and HPC ","date":"2025-06-02","objectID":"/about/:1:4","tags":["intro"],"title":"Welcome to My Blog","uri":"/about/"},{"categories":null,"content":"Get in Touch Feel free to explore my repositories and reach out if you’d like to collaborate on projects or share insights on data science, machine learning, or AI. I’m always open to learning and collaborating with like-minded individuals. You can find me on LinkedIn or via email. Thank you for visiting my GitHub Page! “The best way to predict the future is to create it.” — Abraham Lincoln ","date":"2025-06-02","objectID":"/about/:1:5","tags":["intro"],"title":"Welcome to My Blog","uri":"/about/"},{"categories":["Data Visualization","Tableau","Education"],"content":"Affordable Education in Europe vs. Education Quality: The Best Bang for Your Buck ","date":"2025-03-02","objectID":"/affordable-europe/:1:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization","Tableau","Education"],"content":"Introduction Hello #DataFam! I’m thrilled to share my first-ever personal data visualization project created during my data visualization course at the University of North Texas! In this project, I analyzed a dataset that explores education expenses across European countries, focusing on how affordability correlates with education quality. ","date":"2025-03-02","objectID":"/affordable-europe/:2:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization","Tableau","Education"],"content":"Research Questions Which countries provide the most budget-friendly education options, considering tuition and living costs? What is the average tuition fee across these countries? How does the cost compare to the quality of education provided? ","date":"2025-03-02","objectID":"/affordable-europe/:3:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization","Tableau","Education"],"content":"Key Insights Cost \u0026 Living Expenses Matter: When considering affordable education in Europe, both tuition fees and living costs must be considered together. Most Affordable Countries: Germany leads with the lowest tuition. Followed by Portugal, Spain, Lithuania, Austria, Greece, and Italy. Bulgaria offers the most affordable education overall (low tuition + low cost of living). Most Expensive Countries: The Netherlands, Denmark, Finland, Sweden, and the UK have the highest tuition fees. Ireland, Denmark, and Finland also have high living costs. Average Tuition: Across the countries analyzed, the average tuition fee is around £4,000. Cost ≠ Quality: Affordability doesn’t always guarantee high education quality. Estonia ranked #1 for education quality and access (Score: 91.86) based on a study using 17 intelligence and development indicators. Other top-ranked countries include: Switzerland (84.92) Ireland (84.78) UK (81.90) Finland, Germany, Denmark, Sweden, Belgium, and Slovenia also scored well. ","date":"2025-03-02","objectID":"/affordable-europe/:4:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization","Tableau","Education"],"content":"The Bottom Line Best Value: Germany and Finland offer an ideal balance—solid education without breaking the bank. The Trade-Off: Affordable doesn’t always mean low quality, and expensive doesn’t always mean elite. Hidden Gems: Several countries provide high-quality education at a lower cost, if you know where to look. ","date":"2025-03-02","objectID":"/affordable-europe/:5:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization","Tableau","Education"],"content":"Behind the Scenes This project was developed using Tableau, where I merged two different datasets: One covering tuition fees and living costs. Another ranking education quality based on intelligence, accessibility, and development metrics. This exercise helped me explore data storytelling, and how visual insights can bring clarity to complex trade-offs like cost vs. quality in education. View the full Tableau visualization here ","date":"2025-03-02","objectID":"/affordable-europe/:6:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization","Tableau","Education"],"content":"Final Thoughts I’m still learning, but this project helped me grow my skills in: Data analysis \u0026 storytelling Multi-dataset merging Visualization design Insight communication If you’re passionate about data-driven storytelling, I’d love your feedback, and would be thrilled to connect and exchange ideas. Thank you for reading! ","date":"2025-03-02","objectID":"/affordable-europe/:7:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"}]