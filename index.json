[{"categories":["Machine Learning"],"content":"CNN, Data Handling and MINST dataset In part one we learned basics of pytorch and built a simple linear model. In this part we will build a CNN model, learn how to handle data and use the MINST dataset. ","date":"2025-06-13","objectID":"/itracker_cnn/:1:0","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Machine Learning"],"content":"Convolutional Neural Networks (CNNs) CNNs are a type of neural network that are particularly well-suited for image data. They use convolutional layers to automatically and adaptively learn spatial hierarchies of features from input images. This makes them very effective for tasks such as image classification, object detection, and more. ","date":"2025-06-13","objectID":"/itracker_cnn/:1:1","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Machine Learning"],"content":"A Quick Overview of CNNs and Data Handling in PyTorch Convolutional Neural Networks (CNNs) are powerful tools for image-related tasks, leveraging convolutional layers to learn spatial hierarchies of features. They reduce computational complexity through parameter sharing and process images hierarchically, making them robust to spatial variations. Key Components of CNNs: Convolutional Layer (nn.Conv2d): Detects local features using kernels, strides, and padding. It processes input channels and outputs feature maps. Pooling Layer (nn.MaxPool2d): Downsamples feature maps, reducing parameters and enhancing translational invariance. Fully Connected Layer (nn.Linear): Flattens high-level features for final classification or regression tasks. What are CNNs and Why Use Them? Convolutional Neural Networks (CNNs) are a specialized kind of neural network designed for processing data with a grid-like topology — like images. Unlike traditional fully connected layers, CNNs use convolutional layers that scan small regions (kernels) over the input, capturing important features such as edges, textures, and patterns. Key benefits of CNNs include: Parameter sharing: The same kernel (filter) is applied across the entire image, drastically reducing the number of parameters compared to fully connected layers. Spatial hierarchy: CNNs can learn hierarchical features from simple edges to complex objects as you go deeper. Robustness: Pooling layers reduce spatial size, making the model more resilient to small shifts or distortions in the image. ","date":"2025-06-13","objectID":"/itracker_cnn/:1:2","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Machine Learning"],"content":"CNN Layers: nn.Conv2d and nn.MaxPool2d Concept Explanation: Convolutional Layers (nn.Conv2d): The core building block of CNNs. They apply a convolution operation to the input, sliding a small filter (kernel) over the input data. This helps capture local patterns like edges, textures, etc. Key idea: Parameter sharing (the same filter is used across different parts of the input image), which reduces the number of parameters compared to fully connected layers on images. Input: Typically a 4D tensor of shape (N, C_in, H_in, W_in) where: N: Batch size C_in: Number of input channels (e.g., 3 for RGB images, 1 for grayscale) H_in: Height of the input feature map W_in: Width of the input feature map Output: A 4D tensor of shape (N, C_out, H_out, W_out) where C_out is the number of output channels (number of filters used). H_out and W_out depend on kernel size, stride, and padding. Note Key nn.Conv2d Parameters: in_channels (int): Number of channels in the input image. out_channels (int): Number of channels produced by the convolution (number of filters). kernel_size (int or tuple): Size of the convolving kernel. stride (int or tuple, optional): Stride of the convolution. Default: 1. padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0. dilation (int or tuple, optional): Spacing between kernel elements. Default: 1. groups (int, optional): Number of blocked connections from input channels to output channels (for depthwise separable convolutions, etc.). Default: 1. bias (bool, optional): If True, adds a learnable bias to the output. Default: True. Example: ```Python import torch import torch.nn as nn conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) input_tensor = torch.randn(1, 3, 32, 32) # Batch of 1 RGB image 32x32 output = conv_layer(input_tensor) print(output.shape) # Output shape: (1, 16, 32, 32) ``` Pooling Layers (nn.MaxPool2d): Used to reduce the spatial dimensions (height and width) of the feature maps. This helps reduce computation, control overfitting, and makes the network more robust to variations in the position of features. Max Pooling: Takes the maximum value from a small window (kernel) as it slides over the feature map. Input: Typically a 4D tensor from a convolutional layer. Output: A 4D tensor with reduced H and W dimensions. The number of channels C remains the same. Note Key nn.MaxPool2d Parameters: kernel_size (int or tuple): The size of the window to take a max over. stride (int or tuple, optional): The stride of the window. Default value is kernel_size. padding (int or tuple, optional): Implicit zero padding to be added on both sides. Default: 0. dilation (int or tuple, optional): Controls the spacing between the kernel points. Default: 1. return_indices (bool, optional): If True, will return the max indices along with the outputs. Useful for MaxUnpool2d. Default: False. ceil_mode (bool, optional): When True, will use ceil instead of floor to compute the output shape. Default: False. Example: ```python pool_layer = nn.MaxPool2d(kernel_size=2, stride=2) pooled_output = pool_layer(output) print(pooled_output.shape) # Shape: (1, 16, 16, 16) ``` Data Handling in PyTorch: Efficient data handling is crucial for training CNNs. PyTorch provides tools like torchvision.transforms for preprocessing, torch.utils.data.Dataset for custom datasets, and torch.utils.data.DataLoader for batching and shuffling data. Transforms: Convert images to tensors, resize them, and normalize values for effective training. Dataset: Subclass Dataset to load and preprocess data samples. DataLoader: Wraps datasets for batching, shuffling, and parallel loading. These components streamline the process of building and training CNNs for tasks like image classification and gaze tracking. Example of Data Preprocessing with torchvision.transforms: from torchvision import transforms transform = transforms.Compose([ transforms.Resize((\u003cHEIGHT\u003e, \u003cWIDTH\u003e)),# Replace \u003cHEIGHT\u003e an","date":"2025-06-13","objectID":"/itracker_cnn/:1:3","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Machine Learning"],"content":"Custom Dataset and DataLoader To work with your own images or data, subclass torch.utils.data.Dataset and implement: __len__: Returns dataset size. __getitem__: Returns a single sample (image and label), applying transforms. The DataLoader wraps the dataset and provides batching, shuffling, and parallel loading. ","date":"2025-06-13","objectID":"/itracker_cnn/:1:4","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Machine Learning"],"content":"Mini Project: Simple CNN on MNIST Put it all together by building a simple CNN to classify handwritten digits using the MNIST dataset. Load MNIST dataset with transforms (ToTensor, Normalize). Define a CNN with Conv2d, ReLU, MaxPool layers. Train with CrossEntropyLoss and Adam optimizer. Evaluate accuracy on the test set. Click here to view this notebook in full screen ","date":"2025-06-13","objectID":"/itracker_cnn/:1:5","tags":null,"title":"PyTorch Eye Blink Tracker Pt.2","uri":"/itracker_cnn/"},{"categories":["Machine Learning"],"content":"PyTorch Fundamentals Before delving into the depths of deep learning—pun intended—it’s crucial to grasp the foundational aspects that pave the way. This will be a journal of sorts to document my learning experience as I figure out how to build an eye blink tracker using PyTorch from scratch. ","date":"2025-06-05","objectID":"/itracker/:1:0","tags":null,"title":"PyTorch Eye Blink Tracker Pt.1","uri":"/itracker/"},{"categories":["Machine Learning"],"content":"What is PyTorch? PyTorch is a powerful open-source machine learning library that provides a flexible and efficient platform for building deep learning models. It is particularly well-suited for tasks involving computer vision, natural language processing, and reinforcement learning. PyTorch operates on tensors, akin to numpy arrays but optimized for GPU-accelerated computing. This efficiency boost—up to 100 times faster than CPUs—is due to GPUs’ inherent computational prowess. hugo ","date":"2025-06-05","objectID":"/itracker/:2:0","tags":null,"title":"PyTorch Eye Blink Tracker Pt.1","uri":"/itracker/"},{"categories":["Machine Learning"],"content":"Attributes and Operations: Similar to numpy arrays, tensors support indexing, slicing, reshaping, and mathematical operations. The concept of broadcasting enables simultaneous operations across compatible elements, enhancing computational efficiency. Building Tensors: import torch import numpy as np m = torch.tensor([[1,2,3], [4,5,6]]) print(m) output: tensor([[1, 2, 3], [4, 5, 6]]) Types of Tensors: np_array = np.array([[1,2,3], [7,8,9]]) torch_tensor = torch.from_numpy(np_array) print(f\"Numpy array to tensor: \\n {torch_tensor}\") zeros_tensor = torch.zeros(2,5) print(f\"zeros_tensor: \\n {zeros_tensor}\") one_tesnors = torch.ones(2,5) print(f\"one_tesnors: \\n {one_tesnors}\") rand_tensor = torch.rand(2,3) print(f\" rand_tensor: \\n {rand_tensor}\") randn_tensor = torch.randn(2,3) print(f\"randn_tensor: \\n {randn_tensor}\") float_tensor =torch.tensor([1.0,2.0,3.0]) print(f\"Float tesnor: \\n {float_tensor.dtype}\") int_tensor = torch.tensor([1,2,3,4], dtype=torch.int64) print(f\"Int tensor: \\n {int_tensor.dtype}\") Numpy array to tensor: tensor([[1, 2, 3], [7, 8, 9]]) zeros_tensor: tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) one_tesnors: tensor([[1., 1., 1., 1., 1.], [1., 1., 1., 1., 1.]]) rand_tensor: tensor([[0.7409, 0.5551, 0.9490], [0.6028, 0.1038, 0.5413]]) randn_tensor: tensor([[-1.3301, -0.3333, -0.4628], [-1.0383, -0.8357, -0.2252]]) Float tesnor: torch.float32 Int tensor: torch.int64 Checking the device and shape of tensors: if torch.cuda.is_available(): device = torch.device(\"cuda\") gpu_tensor = torch.tensor([1,2,3], device=device) print(gpu_tensor.device) else: print(\"CUDA is not available, tensor on CPU\") device = torch.device(\"cpu\") # which is the default print(f\"Shape of tensor: {m.shape}\") print(f\"Datatype of tensor:{m.dtype}\") print(f\"Device tensor is on {m.device}\") CUDA is not available, tensor on CPU Shape of tensor: torch.Size([2, 3]) Datatype of tensor:torch.int64 Device tensor is on cpu PyTorch Tensor operations: extracting rows and columns, changing shapes with .view() and .reshape(), and performing element wise and matrix operations (addition, subtraction, multiplication, division, and matrix multiplication) # 1. Tensor creation and basic indexing/slicing tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) print(\"First row: \", tensor[0]) print(\"First column: \", tensor[:, 0]) print(\"Element at (1,2):\", tensor[1, 2]) # 2. Slicing + in-place update sub = tensor[:2, :2] sub[0, :] = 100 print(\"After in-place update:\\n\", tensor) # 3. Random tensor + reshape/flatten t = torch.rand(4, 4) print(\"Original shape:\", t.shape) print(\"Reshaped (2×8):\", t.view(2, 8).shape) print(\"Flattened: \", t.view(-1).shape) # 4. Element-wise arithmetic x = torch.tensor([1., 2., 3.]) y = torch.tensor([4., 5., 6.]) print(\"\\nAdd: \", x + y) print(\"Sub: \", x - y) print(\"Mul: \", x * y) print(\"Div: \", x / y) # 5. Matrix multiplication mat1 = torch.randn(2, 3) mat2 = torch.randn(3, 4) print(\"\\nMatrix multiplication:\\n\", torch.matmul(mat1, mat2)) # 6. Broadcasting examples a = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(\"\\nAdd scalar: \\n\", a + 10) print(\"Add row vector: \\n\", a + torch.tensor([1, 2, 3])) print(\"Add column vector:\\n\", a + torch.tensor([[1], [2]])) # 7. In-place operation x.add_(y) print(\"\\nIn-place add result:\", x) output: First row: tensor([1, 2, 3]) First column: tensor([1, 4, 7]) Element at (1,2): tensor(6) After in-place update: tensor([[100, 100, 3], [ 4, 5, 6], [ 7, 8, 9]]) Original shape: torch.Size([4, 4]) Reshaped (2×8): torch.Size([2, 8]) Flattened: torch.Size([16]) Add: tensor([5., 7., 9.]) Sub: tensor([-3., -3., -3.]) Mul: tensor([ 4., 10., 18.]) Div: tensor([0.2500, 0.4000, 0.5000]) Matrix multiplication: tensor([[ 0.6673, 0.5182, 1.2387, -0.9455], [ 0.0869, 0.3411, 0.6695, -0.4641]]) Add scalar: tensor([[11, 12, 13], [14, 15, 16]]) Add row vector: tensor([[2, 4, 6], [5, 7, 9]]) Add column vector: tensor([[2, 3, 4], [6, 7, 8]]) In-place add result: tensor([5., 7., 9.]) ","date":"2025-06-05","objectID":"/itracker/:2:1","tags":null,"title":"PyTorch Eye Blink Tracker Pt.1","uri":"/itracker/"},{"categories":["Machine Learning"],"content":"Automatic Differentiation: Central to PyTorch’s appeal is its automatic differentiation capability. By tracking operations on tensors marked with requires_grad=True, it computes gradients (.grad)—crucial for understanding how changes in weights affect model output. Clearing gradients prevents unintended accumulation, ensuring accurate subsequent calculations. Torch.nn.Module: Enter nn.Module, the cornerstone of PyTorch neural networks. This template empowers developers to construct models equipped with essential functionalities—a scaffold for innovation in deep learning architectures. Key Components From common layers and activation functions to loss functions that gauge predictive accuracy, PyTorch offers a robust toolkit for constructing and refining neural networks. Optimizers and Training Loop Optimizers, such as SGD and Adam, fine-tune model parameters by minimizing computed gradients. The training loop—comprising forward passes, loss calculation, backpropagation for gradient computation, and model updates—forms the backbone of neural network training. In essence, mastering these fundamentals equips us to embark on building an eye blink tracker. Stay tuned as we translate theory into practice, harnessing PyTorch’s capabilities to innovate in real-world applications. Gradients \u0026 backward propagation x = torch.tensor(2.0, requires_grad=True) w = torch.tensor(3.0, requires_grad=True) b = torch.tensor(1.0, requires_grad=True) y = w * x + b target = torch.tensor(10.0) loss = (y - target)**2 loss.backward(retain_graph=True) print(f\"Gradient of loss w.r.t x: {x.grad}\") print(f\"Gradient of loss w.r.t w: {w.grad}\") print(f\"Gradient of loss w.r.t b: {b.grad}\") # Reset gradients x.grad = None w.grad = None b.grad = None print(f\"Gradients reset for x, w, b: {x.grad}, {w.grad}, {b.grad}\") # Gradients with no tracking with torch.no_grad(): y_no_grad = w * x + b print(f\"y_no_grad requires_grad: {y_no_grad.requires_grad}\") Gradient of loss w.r.t x: -18.0 Gradient of loss w.r.t w: -12.0 Gradient of loss w.r.t b: -6.0 Gradients reset for x, w, b: None, None, None y_no_grad requires_grad: False Exercise: Gradient computation with new tensors a = torch.tensor(4.0, requires_grad=True) b = torch.tensor(5.0, requires_grad=True) c = torch.tensor(6.0, requires_grad=True) d = a * b + c l = d**2 l.backward() print(f\"Gradient of loss w.r.t a: {a.grad}\") print(f\"Gradient of loss w.r.t b: {b.grad}\") print(f\"Gradient of loss w.r.t c: {c.grad}\") # Verify gradients are correct print(f\"Gradient of loss w.r.t a (dl/da): {2 * d * b}\") print(f\"Gradient of loss w.r.t b (dl/db): {2 * d * a}\") print(f\"Gradient of loss w.r.t c (dl/dc): {2 * d}\") # Reset gradients and calculate for new operations a.grad = None b.grad = None c.grad = None d1 = a * b d1.backward() print(f\"Gradient of d1: {a.grad}, {b.grad}\") Output: Gradient of loss w.r.t a: 260.0 Gradient of loss w.r.t b: 208.0 Gradient of loss w.r.t c: 52.0 Gradient of loss w.r.t a (dl/da): 260.0 Gradient of loss w.r.t b (dl/db): 208.0 Gradient of loss w.r.t c (dl/dc): 52.0 Gradient of d1: 5.0, 4.0 Creating a simple neural network (Linear Regression Model) Essentially, a linear regression model is a simple neural network with one layer and no activation function. It learns to map input features to output values by adjusting weights and biases during training. To create a simple neural network using Pytorch, we can follow the a simple structure or template as follows: class YourModelName(nn.Module): def __init__(self): super().__init__() self.layer1 = ... self.layer2 = ... def forward(self, x): x = self.layer1(x) x = self.layer2(x) return x class LinearRegressionModel(nn.Module): def __init__(self, input_dim, output_dim): super(LinearRegressionModel, self).__init__() self.linear = nn.Linear(input_dim, output_dim) def forward(self, x): return self.linear(x) # Instantiate model input_features = 5 output_features = 1 model = LinearRegressionModel(input_features, output_features) # Print model and parameters print(f\"M","date":"2025-06-05","objectID":"/itracker/:2:2","tags":null,"title":"PyTorch Eye Blink Tracker Pt.1","uri":"/itracker/"},{"categories":["Machine Learning"],"content":"Resources: PyTorch Documentation Torch.nn Module Documentation ","date":"2025-06-05","objectID":"/itracker/:2:3","tags":null,"title":"PyTorch Eye Blink Tracker Pt.1","uri":"/itracker/"},{"categories":null,"content":"Let me introduce myself! I’m Etsub Wendwosen Feleke, a Data Science graduate student at the University of North Texas (UNT), with a BA in Economics. I am actively learning and obsessing over Machine Learning and AI. My GitHub is a collection of my projects, experiments, and learning journey in the world of data science. ","date":"2025-06-02","objectID":"/about/:1:0","tags":null,"title":"Welcome to My Blog","uri":"/about/"},{"categories":null,"content":"My Skills: Programming Languages: Python, SQL Data Analysis: Pandas, NumPy, Scikit-learn Data Visualization: Tableau, Power BI Machine Learning: Linear Regression, Random Forest, ARIMA Big Data: PySpark, Hadoop Cloud Platforms: AWS, Google Cloud Tools: Jupyter, Git, Docker, VS Code ","date":"2025-06-02","objectID":"/about/:1:1","tags":null,"title":"Welcome to My Blog","uri":"/about/"},{"categories":null,"content":"Current Focus: I’m working on several exciting projects, including: ITracker: A PyTorch-based eye blink tracker using CNNs, which I’m developing to enhance real-time eye movement analysis. Generative AI: Exploring the capabilities of generative models to create synthetic data and improve machine learning applications. ","date":"2025-06-02","objectID":"/about/:1:2","tags":null,"title":"Welcome to My Blog","uri":"/about/"},{"categories":null,"content":"Interests: I’m a curious learner who loves solving complex problems using data. Learning new ways to analyze and visualize data. I enjoy collaborating with others to tackle real-world challenges using Machine Learning. I also enjoy learning about the latest LLMs and testing their capabilities in various applications. When I’m not coding or analyzing data, I am a serial reader; I love mysteries, thrillers, and I enjoy mostly romantic novels. ","date":"2025-06-02","objectID":"/about/:1:3","tags":null,"title":"Welcome to My Blog","uri":"/about/"},{"categories":null,"content":"My Academic Journey: Current Program: MS in Data Science (Graduating May 2026) Key Courses: Big Data Analytics Information Organization Parallel Computing and HPC ","date":"2025-06-02","objectID":"/about/:1:4","tags":null,"title":"Welcome to My Blog","uri":"/about/"},{"categories":null,"content":"Get in Touch Feel free to explore my repositories and reach out if you’d like to collaborate on projects or share insights on data science, machine learning, or AI. I’m always open to learning and collaborating with like-minded individuals. You can find me on LinkedIn or via email: EtsubFeleke@my.unt.edu . Thank you for visiting my GitHub Page! “If, at first, you do not succeed, call it version 1.0.\" ― Khayri R.R. Woulfe ","date":"2025-06-02","objectID":"/about/:1:5","tags":null,"title":"Welcome to My Blog","uri":"/about/"},{"categories":["Machine Learning"],"content":"As part of my data science journey, I took on a challenge that blends environmental urgency with machine learning: forecasting global CO₂ emissions. Using a Kaggle dataset spanning over 200 years and covering 190+ countries, I explored time-series forecasting techniques to understand historical patterns and predict future emissions. With models like Random Forest and ARIMA, I experimented, learned, and iterated—uncovering both the power and limitations of predictive modeling in the climate domain. ","date":"2025-04-05","objectID":"/co2_predictions/:0:0","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"The Goal: Turning Data into Insight The core research question driving this project was: How can machine learning models be used to predict future trends in CO₂ emissions using historical data from around the world? With climate change at the forefront of global discourse, I wanted to apply machine learning to a real-world problem where understanding trends can influence policy and awareness. My focus was not only on model performance but also on drawing out meaningful patterns from long-term data. ","date":"2025-04-05","objectID":"/co2_predictions/:1:0","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"The Dataset: Two Centuries, One Global Story I used the “CO₂ Emissions by Country (Historical)” Kaggle dataset, which contains: Yearly CO₂ emissions per country (in tons) Country/continent information Emissions growth values and other related attributes Preprocessing began with Exploratory Data Analysis (EDA) using pandas and matplotlib, where I uncovered historical emission surges, country-level emission patterns, and outliers. Visuals like time-series plots and choropleth maps helped make sense of the data on both temporal and geographical scales. ","date":"2025-04-05","objectID":"/co2_predictions/:2:0","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"Data Preparation \u0026 Feature Engineering Before diving into forecasting, I prepared the data through: Log transformation: To handle exponential growth patterns and stabilize variance. Standardization: To scale the data for machine learning models, ensuring fair comparison between features. CO₂ to ppm conversion: Using scientific formulas, I translated tons of CO₂ into estimated atmospheric concentrations—giving emissions context beyond just tonnage. These steps ensured cleaner, more balanced input for the models. ","date":"2025-04-05","objectID":"/co2_predictions/:3:0","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"Modeling: Random Forest vs. ARIMA I implemented two key models: ","date":"2025-04-05","objectID":"/co2_predictions/:4:0","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"Random Forest Regression Random Forest was trained to predict emissions based on temporal and geographic features. It handled non-linearity well and achieved an R² score of 0.87, outperforming other methods in accuracy. ","date":"2025-04-05","objectID":"/co2_predictions/:4:1","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"ARIMA (AutoRegressive Integrated Moving Average) As a classical time-series model, ARIMA captured trends over time and performed reasonably well with proper parameter tuning. Its R² score reached 0.81, though it was more sensitive to preprocessing choices and stationarity assumptions. ","date":"2025-04-05","objectID":"/co2_predictions/:4:2","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"Lessons from Challenges While the models showed promise, forecasting emissions proved more complex than expected. Some challenges included: Inconsistent trends across countries: Some nations saw exponential growth, others a decline. Data gaps and volatility: Not all countries had consistent records, especially in earlier years. Model limitations: ARIMA struggled with noisy historical fluctuations, and even Random Forest couldn’t fully capture sudden global shifts (e.g., policy changes, pandemics). But each obstacle deepened my understanding of what it takes to model complex global phenomena. ","date":"2025-04-05","objectID":"/co2_predictions/:5:0","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"Visualization and Communication Using matplotlib, seaborn, and geospatial libraries, I created: Emission trends per country World maps showing historical and projected CO₂ concentrations Model comparison plots and error visualizations These visuals weren’t just for analysis—they became powerful storytelling tools in communicating findings clearly. ","date":"2025-04-05","objectID":"/co2_predictions/:6:0","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"What’s Next? I plan to extend this project by: Incorporating energy consumption, economic indicators, and industrial activity as new predictors Exploring deep learning approaches like LSTM networks for sequential data Experimenting with XGBoost for robust regression As I continue learning, I’m excited to push this project further—toward actionable, science-driven forecasting. ","date":"2025-04-05","objectID":"/co2_predictions/:7:0","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"Key Takeaways Machine learning offers meaningful tools for climate analytics—but data complexity matters Preprocessing (especially transformations) has a significant impact on model stability There’s no one-size-fits-all model: blending classical and modern methods yields deeper insights ","date":"2025-04-05","objectID":"/co2_predictions/:8:0","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"See It in Action Want to explore the code? I’ve documented everything, including modeling, visualization, and preprocessing steps in this notebook. ","date":"2025-04-05","objectID":"/co2_predictions/:9:0","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"Code Snippets Libraries and Data Loading import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import warnings import numpy as np import pandas as pd import statsmodels.api as sm import matplotlib.pyplot as plt from sklearn.metrics import r2_score from statsmodels.tsa.arima.model import ARIMA from statsmodels.tsa.stattools import adfuller from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.graphics.tsaplots import plot_acf, plot_pacf from sklearn.model_selection import train_test_split warnings.filterwarnings(\"ignore\") #Load the dataset df = pd.read_csv('Year-on-year change in CO emissions.csv') df.head() Data Preprocessing and Initial Exploration df.info() df['Year'] = pd.to_numeric(df['Year'], errors='coerce') df['growth_emissions_total'] = pd.to_numeric(df['growth_emissions_total'], errors='coerce') missing_values = df.isnull().sum() print('Missing Values in Each Column:') print(missing_values) df.dropna(subset=['Year', 'growth_emissions_total'], inplace=True) df.reset_index(drop=True, inplace=True) # 2. Data Visualization # 2. Distribution of emissions by country and year plt.figure(figsize=(12, 6)) sns.histplot(df['growth_emissions_total'], bins=50, kde=True) plt.title(\"Distribution of CO₂ Emissions\") plt.xlabel(\"CO₂ Emissions\") plt.ylabel(\"Frequency\") plt.show() # 3. Trends Over Time # Plot CO₂ emissions over time for each country plt.figure(figsize=(14, 7)) top_countries = df.groupby('Entity')['growth_emissions_total'].sum().nlargest(10).index for country in top_countries: country_data = df[df['Entity'] == country] plt.plot(country_data['Year'], country_data['growth_emissions_total'], label=country) plt.title(\"CO₂ Emissions Over Time (Sample of 10 Countries)\") plt.xlabel(\"Year\") plt.ylabel(\"CO₂ Emissions\") plt.legend() plt.show() # 5. Geographical Trends import plotly.express as px # Create a map visualizing CO₂ contributions by country fig = px.choropleth(df, locations=\"Code\", color=\"growth_emissions_total\", hover_name=\"Entity\", animation_frame=\"Year\", title=\"CO₂ Emissions by Country Over Time\", color_continuous_scale=px.colors.sequential.Plasma) fig.show() #map visualization after the 1950s # Filter data for years after 1950 df_after_1950 = df[df['Year'] \u003e= 1950] # Create a map visualizing CO₂ contributions by country after 1950 fig_after_1950 = px.choropleth(df_after_1950, locations=\"Code\", color=\"growth_emissions_total\", hover_name=\"Entity\", animation_frame=\"Year\", title=\"CO₂ Emissions by Country Over Time (After 1950)\", color_continuous_scale=px.colors.sequential.Plasma) fig_after_1950.show() #compute year-on-year change in CO2 emissions df['year_on_year_change'] = df.groupby('Entity')['growth_emissions_total'].pct_change()*100 df = df.dropna(subset=['year_on_year_change']) # 6. Year-on-Year Change plt.figure(figsize=(12, 6)) sns.histplot(df['year_on_year_change'], bins=50, kde=True) plt.title(\"Year-on-Year Change in CO₂ Emissions\") plt.xlabel(\"Year-on-Year Change (%)\") plt.ylabel(\"Frequency\") plt.grid() plt.show() #total emissions per year total_emissions_per_year = df.groupby('Year')['growth_emissions_total'].sum() plt.figure(figsize=(12, 6)) plt.plot(total_emissions_per_year.index, total_emissions_per_year.values, marker='o') plt.title(\"Total CO₂ Emissions Over Time\") plt.xlabel(\"Year\") plt.ylabel(\"Total CO₂ Emissions\") plt.grid() plt.show() #avergae emissions per country avg_emissions_by_country = df.groupby('Entity')['growth_emissions_total'].mean() print(avg_emissions_by_country) #top 10 countries with highest average emissions top_10_countries = avg_emissions_by_country.nlargest(10) print(\"Top 10 Countries with Highest Average CO₂ Emissions:\") print(top_10_countries) #bottom 10 countries with lowest average emissions bottom_10_countries = avg_emissions_by_country.nsmallest(10) print(\"Bottom 10 Countries with Lowest Average CO₂ Emissions:\") print(bottom_10_countries) Data Transformation (Log, Standardization, Outlier Handling) # Apply log transformat","date":"2025-04-05","objectID":"/co2_predictions/:9:1","tags":null,"title":"Global CO₂ Emissions Predictions: A Machine Learning Approach","uri":"/co2_predictions/"},{"categories":["Machine Learning"],"content":"Learning to Predict Housing Market Trends with Machine Learning In the competitive world of real estate, understanding market trends and forecasting sales performance can lead to smarter investment decisions and better risk planning. As part of my self-guided learning in predictive modeling, I explored housing market data to simulate the kind of analysis a real estate analytics team might perform. Using a global housing market dataset from Kaggle (2015–2024), I applied machine learning techniques to build models that could forecast sales trends and surface key drivers of market movement. The goal was to push beyond theory and create something with real-world relevance—and showcase how data science can inform strategic decisions. ","date":"2025-03-14","objectID":"/predictive_modeling/:1:0","tags":null,"title":"Predictive modeling","uri":"/predictive_modeling/"},{"categories":["Machine Learning"],"content":"The Dataset: Real Estate Signals from Around the World My journey began with exploring a global housing market dataset from Kaggle. This dataset provided a rich source of information, allowing me to delve into various factors influencing housing prices. import pandas as pd # Load the dataset df = pd.read_csv('Global_Housing_Market.csv') # View key features print(df.columns) The dataset included: House Price Index (HPI) Rent Index Affordability Ratio Mortgage Rate (%) Inflation Rate (%) GDP Growth (%) Construction Index ","date":"2025-03-14","objectID":"/predictive_modeling/:1:1","tags":null,"title":"Predictive modeling","uri":"/predictive_modeling/"},{"categories":["Machine Learning"],"content":"The Challenge: Predicting Housing Price Index The core objective of this project was to predict future values of the House Price Index using economic and housing variables. To simulate market demand, I leveraged affordability and rent indices as contextual features. This allowed me to learn how to frame a real-world prediction problem in a machine learning context. ","date":"2025-03-14","objectID":"/predictive_modeling/:1:2","tags":null,"title":"Predictive modeling","uri":"/predictive_modeling/"},{"categories":["Machine Learning"],"content":"Feature Engineering for Smarter Predictions To enhance model performance, I focused on feature engineering, a crucial step in preparing data for machine learning. This involved handling missing values and creating new features from existing ones. # Fill missing values df.fillna(method='ffill', inplace=True) # Create lag features df['HPI_lag1'] = df['House Price Index'].shift(1) df['GDP_growth_lag1'] = df['GDP Growth (%)'].shift(1) # Drop rows with NaN after shifting df.dropna(inplace=True) # Feature set and target features = ['Rent Index', 'Affordability Ratio', 'Mortgage Rate (%)', 'Inflation Rate (%)', 'GDP_growth_lag1', 'Construction Index', 'HPI_lag1'] target = 'House Price Index' X = df[features] y = df[target] Through this process, I learned the importance of transforming raw data into a format that machine learning models can effectively learn from. ","date":"2025-03-14","objectID":"/predictive_modeling/:1:3","tags":null,"title":"Predictive modeling","uri":"/predictive_modeling/"},{"categories":["Machine Learning"],"content":"Model Building with scikit-learn I then moved on to building predictive models. I decided to compare a traditional linear regression model with a more flexible Random Forest Regressor to understand their strengths and weaknesses in this context. from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score, mean_squared_error # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Random Forest rf = RandomForestRegressor(n_estimators=100, random_state=42) rf.fit(X_train, y_train) y_pred_rf = rf.predict(X_test) # Linear Regression lr = LinearRegression() lr.fit(X_train, y_train) y_pred_lr = lr.predict(X_test) # Evaluation print(\"Random Forest R²:\", r2_score(y_test, y_pred_rf)) print(\"Linear Regression R²:\", r2_score(y_test, y_pred_lr)) This hands-on experience taught me how to train, test, and evaluate different regression models using scikit-learn, a fundamental library in machine learning. ","date":"2025-03-14","objectID":"/predictive_modeling/:1:4","tags":null,"title":"Predictive modeling","uri":"/predictive_modeling/"},{"categories":["Machine Learning"],"content":"Communicating Results Visually Finally, I focused on making the model’s insights accessible and impactful through data visualization. This allowed me to understand not just what the model predicted, but why. import matplotlib.pyplot as plt import seaborn as sns # Feature importance from Random Forest importances = rf.feature_importances_ sns.barplot(x=importances, y=features) plt.title(\"Feature Importance\") plt.show() # Predicted vs Actual plt.plot(y_test.values, label=\"Actual\") plt.plot(y_pred_rf, label=\"Predicted\") plt.legend() plt.title(\"Random Forest Predictions vs Actual HPI\") plt.show() These visualizations brought clarity to the models’ decisions and showcased the real-world impact of factors like mortgage rates and construction activity on the House Price Index. ","date":"2025-03-14","objectID":"/predictive_modeling/:1:5","tags":null,"title":"Predictive modeling","uri":"/predictive_modeling/"},{"categories":["Machine Learning"],"content":"What I Learned This project was an invaluable learning experience. It taught me how to: Build end-to-end regression models using real-world data. Handle missing values and apply effective feature transformations. Evaluate and visualize model performance for practical business applications. Utilize model interpretation techniques, such as feature importance, to generate actionable insights. ","date":"2025-03-14","objectID":"/predictive_modeling/:1:6","tags":null,"title":"Predictive modeling","uri":"/predictive_modeling/"},{"categories":["Data Visualization"],"content":"Affordable Education in Europe vs. Education Quality: The Best Bang for Your Buck ","date":"2025-03-02","objectID":"/affordable-europe/:1:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization"],"content":"Introduction Hello #DataFam! I’m thrilled to share my first-ever personal data visualization project created during my data visualization course at the University of North Texas! In this project, I analyzed a dataset that explores education expenses across European countries, focusing on how affordability correlates with education quality. ","date":"2025-03-02","objectID":"/affordable-europe/:2:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization"],"content":"Research Questions Which countries provide the most budget-friendly education options, considering tuition and living costs? What is the average tuition fee across these countries? How does the cost compare to the quality of education provided? ","date":"2025-03-02","objectID":"/affordable-europe/:3:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization"],"content":"Key Insights Cost \u0026 Living Expenses Matter: When considering affordable education in Europe, both tuition fees and living costs must be considered together. Most Affordable Countries: Germany leads with the lowest tuition. Followed by Portugal, Spain, Lithuania, Austria, Greece, and Italy. Bulgaria offers the most affordable education overall (low tuition + low cost of living). Most Expensive Countries: The Netherlands, Denmark, Finland, Sweden, and the UK have the highest tuition fees. Ireland, Denmark, and Finland also have high living costs. Average Tuition: Across the countries analyzed, the average tuition fee is around £4,000. Cost ≠ Quality: Affordability doesn’t always guarantee high education quality. Estonia ranked #1 for education quality and access (Score: 91.86) based on a study using 17 intelligence and development indicators. Other top-ranked countries include: Switzerland (84.92) Ireland (84.78) UK (81.90) Finland, Germany, Denmark, Sweden, Belgium, and Slovenia also scored well. ","date":"2025-03-02","objectID":"/affordable-europe/:4:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization"],"content":"The Bottom Line Best Value: Germany and Finland offer an ideal balance—solid education without breaking the bank. The Trade-Off: Affordable doesn’t always mean low quality, and expensive doesn’t always mean elite. Hidden Gems: Several countries provide high-quality education at a lower cost, if you know where to look. ","date":"2025-03-02","objectID":"/affordable-europe/:5:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization"],"content":"Behind the Scenes This project was developed using Tableau, where I merged two different datasets: One covering tuition fees and living costs. Another ranking education quality based on intelligence, accessibility, and development metrics. This exercise helped me explore data storytelling, and how visual insights can bring clarity to complex trade-offs like cost vs. quality in education. View the full Tableau visualization here ","date":"2025-03-02","objectID":"/affordable-europe/:6:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Data Visualization"],"content":"Final Thoughts I’m still learning, but this project helped me grow my skills in: Data analysis \u0026 storytelling Multi-dataset merging Visualization design Insight communication If you’re passionate about data-driven storytelling, I’d love your feedback, and would be thrilled to connect and exchange ideas. Thank you for reading! ","date":"2025-03-02","objectID":"/affordable-europe/:7:0","tags":null,"title":"Affordable Education in Europe","uri":"/affordable-europe/"},{"categories":["Python"],"content":"Exploring Web Scraping with Beautiful Soup and Requests: A Learning Journey Understanding web scraping techniques is essential for data science. The goal was to extract structured data from a real-world website, specifically focusing on the World Economic Forum’s Wikipedia page. This hands-on experience allowed me to learn how to programmatically interact with web content and parse HTML to gather information. ","date":"2024-11-14","objectID":"/web_scraping/:1:0","tags":null,"title":"Web scraping with Beautiful Soup","uri":"/web_scraping/"},{"categories":["Python"],"content":"The Tools: Requests and Beautiful Soup I chose to use two powerful Python libraries for this project: requests for making HTTP requests to fetch web pages, and BeautifulSoup for parsing the HTML content. First, I used requests to get the content of the Wikipedia page: from bs4 import BeautifulSoup import requests url = 'https://en.wikipedia.org/wiki/World_Economic_Forum' page = requests.get(url) Once the page content was retrieved, I used BeautifulSoup to parse the HTML. This step transforms the raw HTML into a parseable object, making it easy to navigate and search for specific elements. soup = BeautifulSoup(page.text, 'html') print(soup) # This prints the full HTML content of the page ","date":"2024-11-14","objectID":"/web_scraping/:1:1","tags":null,"title":"Web scraping with Beautiful Soup","uri":"/web_scraping/"},{"categories":["Python"],"content":"Navigating and Extracting Data from HTML Tables My primary interest was to extract information presented in tables on the Wikipedia page. I started by identifying the specific table I wanted to work with, which in this case was the “Overview of past annual meetings” table. I found that I could locate the table using its class attribute. # To find the second table on the page print(soup.find_all('table')[1]) # To find the specific table by its class table = soup.find('table', class_='wikitable mw-collapsible') print(table) After locating the table, the next step was to extract the table headers. This was crucial for understanding the structure of the data and for creating a meaningful DataFrame later. table_title = table.find_all('th', scope=\"col\") print(table_title) table_title_col = [title.text.strip() for title in table_title] print(table_title_col) With the column headers extracted, I initialized a pandas DataFrame to store the scraped data. import pandas as pd df = pd.DataFrame(columns=table_title_col) print(df) The most involved part was iterating through the table rows and extracting the data for each cell. I learned that I needed to handle potential inconsistencies in row lengths and properly append data to the DataFrame. row_data = table.find_all('tr')[1:] print(row_data) for row in row_data[1:]: table_data = row.find_all(['td', 'th']) individual_table_data = [data.text.strip() for data in table_data] # Ensure the data matches the number of columns before appending if len(individual_table_data) == len(df.columns): df.loc[len(df)] = individual_table_data else: # Pad missing values if row length is inconsistent individual_table_data += [None] * (len(df.columns) - len(individual_table_data)) df.loc[len(df)] = individual_table_data # This line was causing duplicates, fixed it. # Corrected logic for appending rows to avoid duplicates from original snippet: # Resetting df to re-populate with corrected logic if this were a fresh run df = pd.DataFrame(columns=table_title_col) for row in row_data[1:]: table_data = row.find_all(['td', 'th']) individual_table_data = [data.text.strip() for data in table_data] # Handle cases where some rows might have fewer columns than expected if len(individual_table_data) \u003c len(df.columns): individual_table_data.extend([None] * (len(df.columns) - len(individual_table_data))) df.loc[len(df)] = individual_table_data print(df) print(type(df[[\"Dates\"]])) Finally, I saved the extracted data into a CSV file for further analysis. df.to_csv(r'/Users/etsubfeleke/Documents/practice.csv', index=False) ","date":"2024-11-14","objectID":"/web_scraping/:1:2","tags":null,"title":"Web scraping with Beautiful Soup","uri":"/web_scraping/"},{"categories":["SQL"],"content":"Building a Hotel Management Database System: A Learning Journey In the dynamic world of hospitality, efficient data management is key to seamless operations, from guest reservations to staff scheduling. As part of my hands-on journey to deepen my backend development and SQL skills, I embarked on designing and implementing a comprehensive Hotel Management System. This project wasn’t just about building a system; it was about immersing myself in the practical aspects of relational database design, query optimization, and the intricacies of bringing business logic to life with SQL. This experience gave me invaluable insights into: Relational Database Design: Learning to structure data effectively using normalization principles. SQL Query Optimization: Discovering how to write efficient queries for reporting and daily operations. Stored Procedures (Functions in PostgreSQL): Encapsulating complex business logic for reusability and efficiency. Performance Tuning: Understanding the critical role of indexing and careful query construction in real-world systems. My goal was to build a system capable of managing real-time hotel operations, enhancing efficiency, and ensuring data integrity. ","date":"2024-08-14","objectID":"/dbms_sql/:1:0","tags":null,"title":"Building Database Management System Using SQL","uri":"/dbms_sql/"},{"categories":["SQL"],"content":"Step 1: Designing the Database Schema – My First Dive into Relational Design I started by meticulously designing the database schema, a foundational step that taught me the importance of a well-structured database. I used an Entity-Relationship (ER) diagram, much like the Crow’s Foot model, to visually map out the hotel’s core entities and their relationships. This involved carefully considering: Guests: Basic guest information and their historical interactions. Rooms: Details about each room, its type, rate, and current status. Reservations: Linking guests to rooms for specific periods. Payments: Tracking financial transactions. Staff (Employees) \u0026 Departments: Organizing hotel personnel and their roles. Employee Schedules: Managing work shifts. Guest Reviews: Capturing customer feedback. I focused on applying 3rd Normal Form (3NF) principles to minimize data redundancy and enforce referential integrity. This ensures that data is consistent and reliable across the entire system. Here are the CREATE TABLE statements I learned to write, tailored for a PostgreSQL environment (as initially planned, though my original exploration used MySQL Workbench for visual design and initial scripting): -- Creating the Guests table to store customer information CREATE TABLE Guests ( guest_id SERIAL PRIMARY KEY, guest_name VARCHAR(200) NOT NULL, guest_email VARCHAR(200) NOT NULL, phone_no VARCHAR(20) NOT NULL, -- Using VARCHAR for phone numbers for flexibility address VARCHAR(200) NOT NULL, preferences VARCHAR(200), -- Guest preferences (e.g., non-smoking, high floor) stay_history VARCHAR(200) -- History of previous stays ); -- Creating the Departments table for organizing hotel departments CREATE TABLE Departments ( department_id SERIAL PRIMARY KEY, department_name VARCHAR(200) NOT NULL, number_of_employees INT NOT NULL ); -- Creating the Employees table to store staff details CREATE TABLE Employees ( employee_id SERIAL PRIMARY KEY, employee_name VARCHAR(200) NOT NULL, employee_role VARCHAR(200) NOT NULL, -- e.g., 'Receptionist', 'Manager' employee_phone_no VARCHAR(20) NOT NULL, employee_email VARCHAR(200) NOT NULL, department_id INT NOT NULL, CONSTRAINT fk_department_id FOREIGN KEY (department_id) REFERENCES Departments(department_id) ); -- Creating the Employee_Schedules table to manage staff work shifts CREATE TABLE Employee_Schedules ( schedule_id SERIAL PRIMARY KEY, employee_id INT NOT NULL, schedule_type VARCHAR(50) NOT NULL, -- e.g., 'Full-Time', 'Part-Time', 'Temporary' schedule_date DATE NOT NULL, clock_in_time TIME NOT NULL, CONSTRAINT fk_employee_id FOREIGN KEY (employee_id) REFERENCES Employees(employee_id) ); -- Creating the Rooms table to store room details CREATE TABLE Rooms ( room_no SERIAL PRIMARY KEY, room_type VARCHAR(50) NOT NULL, -- e.g., 'Deluxe', 'Standard', 'Suite' room_price DECIMAL(10,2) NOT NULL, room_category VARCHAR(100) NOT NULL, -- e.g., 'Standard Suite', 'Presidential Suite' room_status VARCHAR(50) NOT NULL, -- e.g., 'Available', 'Occupied', 'Under Maintenance' schedule_id INT NOT NULL, -- This foreign key links to an employee schedule, possibly for cleaning or maintenance assignment CONSTRAINT fk_room_schedule_id FOREIGN KEY (schedule_id) REFERENCES Employee_Schedules(schedule_id) ); -- Creating the Reservations table to manage guest bookings CREATE TABLE Reservations ( reservation_id SERIAL PRIMARY KEY, guest_id INT NOT NULL, room_no INT NOT NULL, check_in_date TIMESTAMP NOT NULL, check_out_date TIMESTAMP NOT NULL, confirmation_no VARCHAR(50) UNIQUE NOT NULL, -- Unique confirmation number for each reservation reservation_status VARCHAR(50) DEFAULT 'Pending', -- e.g., 'Confirmed', 'Pending', 'Cancelled' CONSTRAINT fk_reservation_guest_id FOREIGN KEY (guest_id) REFERENCES Guests(guest_id), CONSTRAINT fk_reservation_room_no FOREIGN KEY (room_no) REFERENCES Rooms(room_no) ); -- Creating the Payments table to record financial transactions CREATE TABLE Payments ( payment_id SERIAL PRIMARY KEY, reservation_id INT NO","date":"2024-08-14","objectID":"/dbms_sql/:1:1","tags":null,"title":"Building Database Management System Using SQL","uri":"/dbms_sql/"},{"categories":["SQL"],"content":"Step 2: Populating the Database – Bringing Data to Life After defining the schema, the next challenge was populating the tables with data. I learned the process of preparing data in CSV files and then importing it into the database. This approach proved efficient for initial data loading and simulated real-world data migration. While the full INSERT statements for all rows are extensive, here’s an example structure for how data would be inserted into one of the tables: -- Example data insertion for the Departments table INSERT INTO Departments (department_id, department_name, number_of_employees) VALUES (1001, 'Housekeeping', 10), (1002, 'Room Service', 15), (1003, 'Reception', 5), (1004, 'Guest Relations', 5), (1005, 'Administration', 2); ","date":"2024-08-14","objectID":"/dbms_sql/:1:2","tags":null,"title":"Building Database Management System Using SQL","uri":"/dbms_sql/"},{"categories":["SQL"],"content":"Step 3: Implementing Business Logic with Stored Procedures and Queries – Beyond Basic SELECTs This phase was particularly enlightening as I moved beyond simple data storage to implementing dynamic business logic. I experimented with stored procedures (functions in PostgreSQL terms) to automate common tasks and ensure data consistency. Here are a couple of functions I learned to create, which encapsulate logic like calculating revenue and checking room availability: -- Function to calculate total revenue within a specified date range CREATE OR REPLACE FUNCTION calculate_revenue(start_date DATE, end_date DATE) RETURNS NUMERIC AS $$ BEGIN RETURN ( SELECT SUM(amount) FROM Payments WHERE payment_date BETWEEN start_date AND end_date ); END; $$ LANGUAGE plpgsql; -- Function to check if a specific room is available for a given period CREATE OR REPLACE FUNCTION is_room_available(room_num INT, start_date DATE, end_date DATE) RETURNS BOOLEAN AS $$ BEGIN RETURN NOT EXISTS ( SELECT 1 FROM Reservations WHERE room_no = room_num AND reservation_status = 'Confirmed' -- Only consider confirmed bookings for availability AND (check_in_date, check_out_date) OVERLAPS (start_date, end_date) ); END; $$ LANGUAGE plpgsql; I also practiced writing various SELECT queries to retrieve specific operational data and generate reports. These queries taught me how to join tables and filter results efficiently: -- Query 1: Check availability of rooms SELECT Room_no, Room_type, Room_price FROM Rooms WHERE Room_status = 'Available'; -- Query 2: Retrieve employee data working in a specific role (e.g., 'Administration') SELECT E.Employee_name, E.Employee_phone_no, D.Department_name, E.Employee_email FROM Employees AS E JOIN Departments AS D ON E.department_id = D.department_id WHERE E.Employee_role = 'Administration'; -- Query 3: Get average ratings for services from guest reviews SELECT ROUND(AVG(Room_quality_rating), 2) AS Avg_Room_Quality, ROUND(AVG(Service_rating), 2) AS Avg_Service, ROUND(AVG(Cleanliness_rating), 2) AS Avg_Cleanliness, ROUND(AVG(Amenities_rating), 2) AS Avg_Amenities FROM Guest_Reviews; -- Query 4: Get the details of employees who work full-time SELECT * FROM Employee_Schedules WHERE Schedule_type = 'Full-Time'; -- Query 5: List the top-rated rooms based on guest reviews SELECT R.Room_no, ROUND(AVG((GR.Room_quality_rating + GR.Service_rating + GR.Cleanliness_rating + GR.Amenities_rating) / 4.0)) AS Average_Rating FROM Guest_Reviews AS GR JOIN Reservations AS Res ON GR.reservation_id = Res.reservation_id JOIN Rooms AS R ON Res.room_no = R.room_no GROUP BY R.Room_no ORDER BY Average_Rating DESC; ","date":"2024-08-14","objectID":"/dbms_sql/:1:3","tags":null,"title":"Building Database Management System Using SQL","uri":"/dbms_sql/"},{"categories":["SQL"],"content":"Step 4: Optimizing for Performance – Making It Fast Understanding that a well-designed schema isn’t enough for performance, I delved into optimization techniques. I learned about: Indexing: Creating indexes on frequently queried columns (like room_no, guest_id, and payment_date) significantly speeds up data retrieval. Query Refinement: Using PostgreSQL’s EXPLAIN and ANALYZE tools helped me understand query execution plans, identify bottlenecks, and rewrite inefficient queries. Materialized Views/Summary Tables: For complex or frequently run reports, I learned to consider creating pre-computed summary tables to reduce live query load. These improvements were crucial for achieving a significant reduction in query runtime for common reports, ultimately enhancing the system’s responsiveness. ","date":"2024-08-14","objectID":"/dbms_sql/:1:4","tags":null,"title":"Building Database Management System Using SQL","uri":"/dbms_sql/"},{"categories":["SQL"],"content":"Key Takeaways This project provided me with invaluable end-to-end experience in database development. I gained a deep appreciation for: Schema Design: The art of structuring data for efficiency and integrity. SQL Fluency: Writing precise queries and understanding advanced SQL features like functions. Database Management Systems: Practical interaction with PostgreSQL and its optimization tools. Translating Requirements: Converting real-world business operations into efficient data structures and logic. ","date":"2024-08-14","objectID":"/dbms_sql/:1:5","tags":null,"title":"Building Database Management System Using SQL","uri":"/dbms_sql/"},{"categories":["SQL"],"content":"Why This Matters In today’s data-driven world, the ability to design, optimize, and query relational systems is a foundational skill for any data professional. This project showcases my strong grasp of SQL and PostgreSQL, and my practical experience in improving system performance through thoughtful database design. It’s a testament to how hands-on coding and tool exploration directly translate into a deeper understanding of complex systems. ` ","date":"2024-08-14","objectID":"/dbms_sql/:1:6","tags":null,"title":"Building Database Management System Using SQL","uri":"/dbms_sql/"}]